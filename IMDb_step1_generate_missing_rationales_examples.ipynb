{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(2019)\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'D:\\\\python_pkg_data\\\\huggingface\\\\transformers'\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import numpy as np \n",
    "np.random.seed(2019)\n",
    "import random\n",
    "random.seed(2019)\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(2019)\n",
    "torch.cuda.manual_seed_all(2019)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import transformers\n",
    "\n",
    "\n",
    "\n",
    "from datasets import load_metric,load_dataset,Value\n",
    "import csv\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.data.path.append('D:\\\\python_pkg_data\\\\nltk_data')\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "import ast\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "import importlib\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Softmax\n",
    "from termcolor import colored\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import html\n",
    "from IPython.core.display import display, HTML\n",
    "import more_itertools as mit\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'ori_train_dir':'./datasets/IMDb/orig/train.tsv',\n",
    "    'gpu_device':0,\n",
    "    'tokenizer': transformers.AutoTokenizer.from_pretrained('roberta-base'),\n",
    "    'dataset_cache_dir':\"D:\\\\python_pkg_data\\\\huggingface\\\\Datasets\", ## local directory for datasets\n",
    "    'train_random_seed':2019,                                        ## random seed for subsampling training set\n",
    "    'num_per_class': 25,                                              ## number of examples per class for initial training set\n",
    "    'save_dir': './SF_results/IMDb_step0_sf_trainer',                                   ##directory for saving models\n",
    "    'num_per_step':50,                                                ##num labelled data per step\n",
    "    'num_per_example':7,\n",
    "    'labelled_pos': './datasets/IMDb/human_labelled/positives.json',\n",
    "    'labelled_neg':'./datasets/IMDb/human_labelled/negatives.json',\n",
    "    'supplement': './datasets/IMDb/human_labelled/supplement_rationales.tsv',\n",
    "    'al_dir': './AL_results/AL_step0_IMDb_trainer'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def html_escape(text):\n",
    "    return html.escape(text)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "class CustomerDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "\n",
    "def import_data(directory):\n",
    "    \n",
    "    data_pos = []\n",
    "    data_neg = []\n",
    "    with open(directory,errors='ignore') as file:\n",
    "        file = csv.reader(file, delimiter=\"\\t\")\n",
    "        for idx,row in enumerate(file):\n",
    "            if idx!=0:\n",
    "                if row[0] == 'Negative':\n",
    "                    data_neg.append({'idx':idx,'text':row[1],'label':0})\n",
    "                else:\n",
    "                    data_pos.append({'idx':idx,'text':row[1],'label':1})\n",
    "            \n",
    "    return data_neg,data_pos\n",
    "\n",
    "def import_paired_data(directory,original_texts):\n",
    "    paired_data = {}\n",
    "    with open(directory,errors='ignore') as file:\n",
    "        file = csv.reader(file, delimiter=\"\\t\")\n",
    "        for idx,row in enumerate(file):\n",
    "           \n",
    "            if idx!=0:\n",
    "                if row[2] not in paired_data.keys():\n",
    "                    paired_data[row[2]] = []\n",
    "\n",
    "                    if row[1] in original_texts:\n",
    "                        paired_data[row[2]].append({'text':row[1],'label':0 if row[0]=='Negative'else 1,'ori_flag':1})\n",
    "                      \n",
    "                    else:\n",
    "                        paired_data[row[2]].append({'text':row[1],'label':0 if row[0]=='Negative'else 1,'ori_flag':0})\n",
    "                else:\n",
    "                    if row[1] in original_texts:\n",
    "                        paired_data[row[2]].append({'text':row[1],'label':0 if row[0]=='Negative'else 1,'ori_flag':1})\n",
    "                    else:\n",
    "                        paired_data[row[2]].append({'text':row[1],'label':0 if row[0]=='Negative'else 1,'ori_flag':0})\n",
    "                        \n",
    "    return paired_data\n",
    "\n",
    "def visualise_rationales(original,rationale_spans,rationale_pos,visualise_all=False):\n",
    "    \n",
    "    if visualise_all:\n",
    "        highlighted = []\n",
    "        for idx,term in enumerate(word_tokenize(original)):\n",
    "            if idx in rationale_pos:\n",
    "                highlighted.append(colored(term,'blue'))\n",
    "            else:\n",
    "                highlighted.append(term)\n",
    "            \n",
    "        return TreebankWordDetokenizer().detokenize(highlighted)\n",
    "                \n",
    "    else:\n",
    "        highlights = []\n",
    "        for span in rationale_spans:\n",
    "            highlighted = []\n",
    "            for idx,term in enumerate(word_tokenize(original)):\n",
    "                if idx in span:\n",
    "                    highlighted.append(colored(term,'blue'))\n",
    "                else:\n",
    "                    highlighted.append(term)\n",
    "                    \n",
    "            highlights.append(TreebankWordDetokenizer().detokenize(highlighted))\n",
    "        \n",
    "        return highlights\n",
    "    \n",
    "def visualise_rationales_html(original,rationale_spans,rationale_pos,visualise_all=False):\n",
    "    \n",
    "    if visualise_all:\n",
    "        highlighted = []\n",
    "        for idx,term in enumerate(word_tokenize(original)):\n",
    "            if idx in rationale_pos:\n",
    "                highlighted.append('<font color=\"blue\">' + html_escape(term) + '</font>')\n",
    "            else:\n",
    "                highlighted.append(term)\n",
    "            \n",
    "        return TreebankWordDetokenizer().detokenize(highlighted)\n",
    "                \n",
    "    else:\n",
    "        highlights = []\n",
    "        for span in rationale_spans:\n",
    "            highlighted = []\n",
    "            for idx,term in enumerate(word_tokenize(original)):\n",
    "                if idx in span:\n",
    "                    highlighted.append('<font color=\"blue\">' + html_escape(term) + '</font>')\n",
    "                else:\n",
    "                    highlighted.append(term)\n",
    "                    \n",
    "            highlights.append(TreebankWordDetokenizer().detokenize(highlighted))\n",
    "        \n",
    "        return highlights\n",
    "    \n",
    "\n",
    "def detect_rationale_spans(non_rationale_pos,text_length,max_length=1):\n",
    "    \n",
    "    rationale_spans = []\n",
    "    \n",
    "    rationale_pos = list(set([i for i in range(text_length)])-set(non_rationale_pos))\n",
    "    rationale_pos.sort()\n",
    " \n",
    "    for k, g in groupby(enumerate(rationale_pos),lambda ix : ix[0] - ix[1]): \n",
    "        span = list(map(itemgetter(1), g))\n",
    "        if len(span) <= max_length:\n",
    "            rationale_spans.append(span)\n",
    "    \n",
    "    \n",
    "      \n",
    "    return rationale_spans, rationale_pos\n",
    "\n",
    "def identify_important_terms(token_text):\n",
    "    candidates = []\n",
    "    remove_terms = {}\n",
    "    count = 0\n",
    "    for idx,token in enumerate(token_text):\n",
    "        duplicate = token_text.copy()\n",
    "        remove_terms[count] = {'terms':duplicate[idx],'start_token':idx,'end_token':idx+1}\n",
    "        del duplicate[idx]\n",
    "        count += 1\n",
    "        candidates.append(TreebankWordDetokenizer().detokenize(duplicate))\n",
    "\n",
    "    for idx,token in enumerate(token_text[:-1]):\n",
    "        duplicate = token_text.copy()\n",
    "        remove_terms[count] = {'terms':duplicate[idx:idx+2],'start_token':idx,'end_token':idx+2}\n",
    "        del duplicate[idx:idx+2]\n",
    "        count += 1\n",
    "        candidates.append(TreebankWordDetokenizer().detokenize(duplicate))\n",
    "\n",
    "    for idx,token in enumerate(token_text[:-2]):\n",
    "        duplicate = token_text.copy()\n",
    "        remove_terms[count] = {'terms':duplicate[idx:idx+3],'start_token':idx,'end_token':idx+3}\n",
    "        del duplicate[idx:idx+3]\n",
    "        count += 1\n",
    "        candidates.append(TreebankWordDetokenizer().detokenize(duplicate))\n",
    "\n",
    "\n",
    "    candidates.append(text)\n",
    "    \n",
    "    return candidates, remove_terms\n",
    "\n",
    "\n",
    "def get_rationale_spans(model,text,label,topk=15):\n",
    "    \n",
    "#     text = imdb_texts[example_idx]\n",
    "#     label = imdb_labels[example_idx]\n",
    "    token_text = word_tokenize(text)\n",
    "\n",
    "    candidates, remove_terms = identify_important_terms(token_text)\n",
    "\n",
    "    candidates_label = [label]*len(candidates)\n",
    "\n",
    "    candidates_encodings = args['tokenizer'](candidates, truncation=True, padding=True)\n",
    "    candidates_dataset = CustomerDataset(candidates_encodings, candidates_label)\n",
    "    candidates_dataloader = DataLoader(candidates_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    output_logits = []\n",
    "    for batch in tqdm_notebook(candidates_dataloader):\n",
    "        batch = {k: v.cuda(args['gpu_device']) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        output_logits.append(logits)\n",
    "\n",
    "    outputs = torch.cat(output_logits)\n",
    "    sm = Softmax(dim=1)\n",
    "    outputs = sm(outputs)\n",
    "\n",
    "    results = {}\n",
    "    for idx,score in enumerate(outputs[:-1]):\n",
    "        changes = abs(float(outputs[idx][label]-outputs[-1][label]))\n",
    "        results[idx]=changes\n",
    "\n",
    "    token_id = list(dict(sorted(results.items(), key=lambda item: item[1],reverse=True)).keys())\n",
    "\n",
    "    inferred_spans = []\n",
    "    for ids in token_id[:topk]:\n",
    "#         print(ids, remove_terms[ids]['terms'])\n",
    "        span = [i for i in range(remove_terms[ids]['start_token'],remove_terms[ids]['end_token'])]\n",
    "        inferred_spans.append(span)\n",
    "\n",
    "    inferred_pos = []\n",
    "    for span in inferred_spans:\n",
    "        for number in span:\n",
    "            inferred_pos.append(number)\n",
    "\n",
    "\n",
    "    inferred_pos = list(set(inferred_pos))\n",
    "    \n",
    "    return inferred_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import train data\n",
    "IMDb_data = {}\n",
    "\n",
    "with open(args['ori_train_dir'],errors='ignore') as file:\n",
    "    file = csv.reader(file, delimiter=\"\\t\")\n",
    "    for idx,row in enumerate(file):\n",
    "        if len(row)>0:\n",
    "\n",
    "            if row[0] == 'Negative':\n",
    "                IMDb_data[row[2]] = {'text':row[1],'label':0}\n",
    "            else:\n",
    "                IMDb_data[row[2]] = {'text':row[1],'label':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_rationales = json.load(open(args['labelled_pos'], 'r'))\n",
    "neg_rationales = json.load(open(args['labelled_neg'], 'r'))\n",
    "\n",
    "rationale_spans = {}\n",
    "for item in neg_rationales:\n",
    "    key = list(item.keys())[1]\n",
    "    index = list(item.keys())[1][9:-4]\n",
    "    rationale_spans[index] = item[key]\n",
    "    \n",
    "\n",
    "for item in pos_rationales:\n",
    "    key = list(item.keys())[1]\n",
    "    index = list(item.keys())[1][9:-4]\n",
    "    rationale_spans[index] = item[key]\n",
    "\n",
    "rationale_positions = {}\n",
    "\n",
    "train_keys = list(rationale_spans.keys())\n",
    "\n",
    "for key in train_keys:\n",
    "    doc_positions = []\n",
    "    positions = rationale_spans[key]\n",
    "    for span in positions:\n",
    "        start = span['start_token']\n",
    "        end = span['end_token']\n",
    "        doc_positions = doc_positions +[i for i in range(start,end)]\n",
    "        \n",
    "    rationale_positions[key] = doc_positions\n",
    "\n",
    "len(rationale_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplement_rationales = {}\n",
    "with open(args['supplement'],'r') as file:\n",
    "    file = csv.reader(file, delimiter='\\t')\n",
    "    for idx,row in enumerate(file):\n",
    "        supplement_rationales[row[0]] = ast.literal_eval(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rationale_positions.update(supplement_rationales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most uncertainty examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_keys_dir = f\"{args['save_dir']}_{args['train_random_seed']}_{args['num_per_class']}_{args['num_per_example']}/keys.txt\"\n",
    "new_keys_dir = f\"{args['save_dir']}_{args['train_random_seed']}_{args['num_per_class']}_{args['num_per_example']}/new_keys.txt\"\n",
    "\n",
    "all_keys = []\n",
    "with open(train_keys_dir,'r') as file:\n",
    "    for key in file.readlines():\n",
    "        all_keys.append(key[:-1])\n",
    "        \n",
    "with open(new_keys_dir,'r') as file:\n",
    "    for key in file.readlines():\n",
    "        all_keys.append(key[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Causal Terms via Fine-tuned RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = glob.glob(f\"{args['save_dir']}_{args['train_random_seed']}_{args['num_per_class']}_{args['num_per_example']}/checkpoint*\")[0]\n",
    "print(f\"previous model {model_dir}\")\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_dir, num_labels=2).cuda(args['gpu_device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key = '2589'\n",
    "\n",
    "\n",
    "# text = IMDb_data[key]['text']\n",
    "# label = IMDb_data[key]['label']\n",
    "# token_text = word_tokenize(text)\n",
    "\n",
    "\n",
    "# print(key, 'negative' if label==0 else 'positive')\n",
    "\n",
    "# generated_rationales_spans = get_rationale_spans(model,text,label)\n",
    "\n",
    "\n",
    "\n",
    "# highlighted = visualise_rationales(text,_,generated_rationales_spans,visualise_all=True)\n",
    "# print(highlighted)\n",
    "\n",
    "# rationale_pos = rationale_positions[key]\n",
    "\n",
    "\n",
    "# highlighted = visualise_rationales(text,_,rationale_pos,visualise_all=True)\n",
    "# print(highlighted)\n",
    "\n",
    "# missing_rationales = list(set(rationale_pos) - set(generated_rationales_spans))\n",
    "# missing_rationales.sort()\n",
    "\n",
    "# missing_rationale_pos = [list(group) for group in mit.consecutive_groups(missing_rationales)]\n",
    "\n",
    "# missing_rationale_token = [TreebankWordDetokenizer().detokenize(token_text[row[0]:row[-1]+1]) for row in missing_rationale_pos]\n",
    "\n",
    "# missing_rationale_token\n",
    "\n",
    "# selected_sentence = []\n",
    "# sentences  = sent_tokenize(text)\n",
    "# for idx,sent in enumerate(sentences):\n",
    "#     for token in missing_rationale_token:\n",
    "#         if token in sent:\n",
    "#             selected_sentence.append(sent)\n",
    "#             break\n",
    "\n",
    "# selected_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokened_text = word_tokenize(text)\n",
    "# for idx,token in enumerate(tokened_text):\n",
    "#     if token == \"pain\":\n",
    "#         print(idx, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokened_text[79:82]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "augmented_data ={}\n",
    "for example_idx in tqdm_notebook(all_keys[:]):\n",
    "    \n",
    "    \n",
    "    \n",
    "    text = IMDb_data[example_idx]['text']\n",
    "    label = IMDb_data[example_idx]['label']\n",
    "    token_text = word_tokenize(text)\n",
    "    \n",
    "    print(example_idx, 'negative' if label==0 else 'positive')\n",
    "    \n",
    "    generated_rationales_spans = get_rationale_spans(model,text,label)\n",
    "    \n",
    "    \n",
    "    \n",
    "    highlighted = visualise_rationales(text,_,generated_rationales_spans,visualise_all=True)\n",
    "    print(highlighted)\n",
    "\n",
    "\n",
    "\n",
    "    ## specify non-rationale spans\n",
    "#     non_rationale_pos = non_rationales[example_idx]\n",
    "\n",
    "\n",
    "    ## detect rationale spans\n",
    "#     rationale_spans,rationale_pos = detect_rationale_spans(non_rationale_pos,len(token_original),max_length=1000)\n",
    "    rationale_pos = rationale_positions[example_idx]\n",
    "    highlighted = visualise_rationales(text,_,rationale_pos,visualise_all=True)\n",
    "    print(highlighted)\n",
    "\n",
    "\n",
    "    missing_rationales = list(set(rationale_pos) - set(generated_rationales_spans))\n",
    "    missing_rationales.sort()\n",
    "    missing_rationale_pos = [list(group) for group in mit.consecutive_groups(missing_rationales)]\n",
    "    missing_rationale_token = [TreebankWordDetokenizer().detokenize(token_text[row[0]:row[-1]+1]) for row in missing_rationale_pos]\n",
    "\n",
    "    selected_sentence = []\n",
    "    sentences  = sent_tokenize(text)\n",
    "    for idx,sent in enumerate(sentences):\n",
    "        for token in missing_rationale_token:\n",
    "            if token in sent:\n",
    "                selected_sentence.append(sent)\n",
    "                break\n",
    "\n",
    "    augmented_data[example_idx] = {'candidates':selected_sentence,'label':label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir =  f\"{args['save_dir']}_{args['train_random_seed']}_{args['num_per_class']}_{args['num_per_example']}/missing_rationales_augmented_step1.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_dir, \"w\") as file_name:\n",
    "    json.dump(augmented_data, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output_dir = f\"{args['save_dir']}_{args['train_random_seed']}_{args['num_per_class']}_{args['num_per_example']}/false_rationles.tsv\"\n",
    "# with open(output_dir, 'wt',errors='ignore',newline='') as out_file:\n",
    "#     tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "#     tsv_writer.writerow(['Batch','Positions'])\n",
    "#     for term in rationales:\n",
    "#         tsv_writer.writerow([term['Batch'],term['Positions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
