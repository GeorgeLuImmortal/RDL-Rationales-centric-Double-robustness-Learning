{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,json,random,torch,csv,nltk,math,string,ast\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np \n",
    "from dotted_dict import DottedDict\n",
    "nltk.data.path.append('D:\\\\python_pkg_data\\\\nltk_data')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from termcolor import colored\n",
    "from transformers import AutoTokenizer,pipeline\n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(2020)\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'D:\\\\python_pkg_data\\\\huggingface\\\\transformers'\n",
    "np.random.seed(2020)\n",
    "random.seed(2020)\n",
    "torch.manual_seed(2020)\n",
    "torch.cuda.manual_seed_all(2020)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## replace certian text with [mask] token\n",
    "def Make_masked_text(token_text,replace_id,tokenizer):\n",
    "    \n",
    "    token_text[replace_id] = tokenizer.mask_token\n",
    "    mask_text = TreebankWordDetokenizer().detokenize(token_text)\n",
    "    return mask_text\n",
    "\n",
    "## return all synonym candidates\n",
    "def Get_synonyms(mask_text,unmasker,ban_list):\n",
    "    \n",
    "    synonym_list = []\n",
    "    \n",
    "    for item in unmasker(mask_text):\n",
    "        synonym = item['token_str'].lower().strip()\n",
    "        if synonym not in ban_list:\n",
    "             synonym_list.append(item['token_str'].strip())\n",
    "    \n",
    "\n",
    "    return synonym_list\n",
    "\n",
    "## generate several semi-facutal examples for each document\n",
    "def Generate_semi_factual(text,unmasker,tokenizer,rationale_span,ban_list,percentage=0.15,num_candidates=5):\n",
    "\n",
    "    candidates = []\n",
    "    token_text = word_tokenize(text)\n",
    "    all_ids = [i for i in range(len(token_text))]\n",
    "    \n",
    "    ## ensure the tokens being replaced are not punctuations and rationale tokens\n",
    "    punc_ids = [idx for idx,token in enumerate(token_text) if token in string.punctuation]\n",
    "    non_rationale_ids = list(set(all_ids)-set(punc_ids)-set(rationale_span))\n",
    "    \n",
    "    ## num of tokens being replaced\n",
    "    num_replace = math.ceil(percentage*len(non_rationale_ids))\n",
    "    \n",
    "    ## synonyms of each position inferenced by language models, to reduce prediction time\n",
    "    synonyms = {}\n",
    "   \n",
    "    \n",
    "    for i in tqdm_notebook(range(num_candidates)):\n",
    "        \n",
    "        token_candidate = token_text.copy()\n",
    "        \n",
    "        ## random select non-rationale tokens\n",
    "        replace_ids = random.sample(non_rationale_ids,num_replace)\n",
    "           \n",
    "                \n",
    "        for replace_id in replace_ids:\n",
    "            ## if the synonyms of this position have been predicted by the language models before, direct use\n",
    "            if replace_id in synonyms.keys():\n",
    "                \n",
    "                ## random select one word from the synonym list\n",
    "                if len(synonyms[replace_id])>=1:\n",
    "                    synonym = random.choice(synonyms[replace_id])\n",
    "                    token_candidate[replace_id] = synonym\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "            ## predict the synonyms given this position\n",
    "                mask_text = Make_masked_text(token_text.copy(),replace_id,tokenizer)\n",
    "              \n",
    "                ## get synonyms by mask-filling prediction, ensure the synonym words are not sentimental words or the original token\n",
    "                synonyms[replace_id] = Get_synonyms(mask_text,unmasker,ban_list+[token_text[replace_id]])\n",
    "                \n",
    "                if len(synonyms[replace_id])>=1:\n",
    "                \n",
    "                    synonym = random.choice(synonyms[replace_id])\n",
    "                      \n",
    "                    token_candidate[replace_id] = synonym\n",
    "                    \n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "        candidates.append(TreebankWordDetokenizer().detokenize(token_candidate))\n",
    "        \n",
    "        \n",
    "    return candidates\n",
    "\n",
    "\n",
    "def Random_replacement(text,unmasker,tokenizer,ban_list,percentage=0.15,num_candidates=5):\n",
    "\n",
    "    candidates = []\n",
    "    token_text = word_tokenize(text)\n",
    "    \n",
    "    punc_ids = [idx for idx,token in enumerate(token_text) if token in string.punctuation]\n",
    "    non_punc_ids = list(set([i for i in range(len(token_text))])-set(punc_ids))\n",
    "    \n",
    "    \n",
    "    num_replace = math.ceil(0.15*len(token_text))\n",
    "    synonyms = {}\n",
    "   \n",
    "    \n",
    "    for i in tqdm_notebook(range(num_candidates)):\n",
    "        ## ensure the tokens being replaced are not punctuations\n",
    "        token_candidate = token_text.copy()\n",
    "        \n",
    "        replace_ids = random.sample(non_punc_ids,num_replace)\n",
    "           \n",
    "                \n",
    "        for replace_id in replace_ids:\n",
    "            if replace_id in synonyms.keys():\n",
    "                if len(synonyms[replace_id])>=1:\n",
    "                    synonym = random.choice(synonyms[replace_id])\n",
    "                    token_candidate[replace_id] = synonym\n",
    "                else:\n",
    "                    pass\n",
    "            else:\n",
    "                mask_text = Make_masked_text(token_text.copy(),replace_id,tokenizer)\n",
    "              \n",
    "                synonyms[replace_id] = Get_synonyms(mask_text,unmasker,ban_list+[token_text[replace_id]])\n",
    "                \n",
    "                if len(synonyms[replace_id])>=1:\n",
    "                \n",
    "                    synonym = random.choice(synonyms[replace_id])\n",
    "                      \n",
    "                    token_candidate[replace_id] = synonym\n",
    "                    \n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "        candidates.append(TreebankWordDetokenizer().detokenize(token_candidate))\n",
    "        \n",
    "        \n",
    "    return candidates\n",
    "\n",
    "\n",
    "def Visualise_rationales(original,rationale_spans,rationale_pos,visualise_all=False):\n",
    "    \n",
    "    if visualise_all:\n",
    "        highlighted = []\n",
    "        for idx,term in enumerate(word_tokenize(original)):\n",
    "            if idx in rationale_pos:\n",
    "                highlighted.append(colored(term,'blue'))\n",
    "            else:\n",
    "                highlighted.append(term)\n",
    "            \n",
    "        return TreebankWordDetokenizer().detokenize(highlighted)\n",
    "                \n",
    "    else:\n",
    "        highlights = []\n",
    "        for span in rationale_spans:\n",
    "            highlighted = []\n",
    "            for idx,term in enumerate(word_tokenize(original)):\n",
    "                if idx in span:\n",
    "                    highlighted.append(colored(term,'blue'))\n",
    "                else:\n",
    "                    highlighted.append(term)\n",
    "                    \n",
    "            highlights.append(TreebankWordDetokenizer().detokenize(highlighted))\n",
    "        \n",
    "        return highlights\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'data_dir':'./datasets/IMDb/orig/train.tsv',\n",
    "    'pos_dict_dir':'./datasets/positive-words.txt',\n",
    "    'neg_dict_dir':'./datasets/negative-words.txt',\n",
    "    'pos_labels_dir': './datasets/IMDb/human_labelled/positives.json',\n",
    "    'neg_labels_dir':'./datasets/IMDb/human_labelled/negatives.json',\n",
    "    'supplement':'./datasets/IMDb/human_labelled/supplement_rationales.tsv',\n",
    "    'is_random_replace':False\n",
    "}\n",
    "\n",
    "args = DottedDict(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load datasets, positive words and negative words dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load original data\n",
    "\n",
    "data = {} ##  description:{'doc_id': {'text', 'label '}}\n",
    "          ##              e.g. {'4': {'text': 'Long, boring, blasphemous. Never have I been so glad to see ending credits roll.', 'label': 0}}\n",
    "with open(args.data_dir,errors='ignore') as file:\n",
    "    file = csv.reader(file, delimiter=\"\\t\")\n",
    "    for idx,row in enumerate(file):\n",
    "        if len(row)>0:\n",
    "            if row[0] == 'Negative':\n",
    "                data[row[2]] = {'text':row[1],'label':0}\n",
    "            else:\n",
    "                data[row[2]] = {'text':row[1],'label':1}\n",
    "\n",
    "\n",
    "## a list of positive words and negative words uses as a ban list\n",
    "positive_terms = []            \n",
    "with open(args.pos_dict_dir,'r') as file:\n",
    "    positive_terms=file.read().splitlines()\n",
    "    \n",
    "negative_terms = []            \n",
    "with open(args.neg_dict_dir,'r') as file:\n",
    "    negative_terms=file.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load rationale annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_rationales = json.load(open(args.pos_labels_dir, 'r'))\n",
    "neg_rationales = json.load(open(args.neg_labels_dir, 'r'))\n",
    "\n",
    "\n",
    "rationale_spans = {} # rationales spans of each document {'doc_id':[{'rationale1_id': 'start_position','end_position','text'},...]}\n",
    "                     #                               e.g. {'1006': [{'token_id': 1, 'start_token': 0, 'end_token': 1, 'text': 'SUcks'},\n",
    "\n",
    "for item in neg_rationales:\n",
    "    key = list(item.keys())[1]\n",
    "    index = list(item.keys())[1][9:-4]\n",
    "    rationale_spans[index] = item[key]\n",
    "    \n",
    "\n",
    "for item in pos_rationales:\n",
    "    key = list(item.keys())[1]\n",
    "    index = list(item.keys())[1][9:-4]\n",
    "    rationale_spans[index] = item[key]\n",
    "\n",
    "rationale_positions = {} # rationales positions of each docuemnt {'doc_id':[pos1, pos2, pos3, ...],...]}\n",
    "                        #  '1006': [0, 48, 49, 53, 70, 74, 75, 17, 19, 21, 34, 83, 85, 87],\n",
    "\n",
    "doc_ids = list(rationale_spans.keys())\n",
    "\n",
    "for doc_id in doc_ids:\n",
    "    doc_positions = []\n",
    "    positions = rationale_spans[doc_id]\n",
    "    for span in positions:\n",
    "        start = span['start_token']\n",
    "        end = span['end_token']\n",
    "        doc_positions = doc_positions +[i for i in range(start,end)]\n",
    "        \n",
    "    rationale_positions[doc_id] = doc_positions\n",
    "    \n",
    "## add supplement rationale positions\n",
    "supplement_rationales = {}\n",
    "with open(args['supplement'],'r') as file:\n",
    "    file = csv.reader(file, delimiter='\\t')\n",
    "    for idx,row in enumerate(file):\n",
    "        supplement_rationales[row[0]] = ast.literal_eval(row[1])\n",
    "        \n",
    "rationale_positions.update(supplement_rationales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# visualise rationals of random examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_id:21915\n",
      "doc_label:positive\n",
      "I think this movie is a \u001b[34mvery\u001b[0m \u001b[34mfunny\u001b[0m film and one of the \u001b[34mbest\u001b[0m 'National Lampoon's' films, it also has a \u001b[34mvery\u001b[0m \u001b[34mcatchy\u001b[0m spoof title, which basically sums up what the whole movie is about .... Men In White!!!! The story is a spoof of many films including a Will Smith film, as you might have guessed, 'Men In Black' . I will not give the ending away but it has a \u001b[34mvery\u001b[0m \u001b[34mgood\u001b[0m ending in is very funny (Leslie Nielsen style humour) from start to finish, especially the bit near the beginning when thy are in the street collecting the dustbins (Garbage Cans). Also, they have a \u001b[34mpretty\u001b[0m \u001b[34mcool\u001b[0m dustbin lorry (Garbage Collecting Truck) in that scene too . The acting is not superb, actually, it is not very good, but that is what makes the film \u001b[34mfunny\u001b[0m, it is a comedy, loosen up!! I love the story line, partly because it is so far fetched and partly because it is interesting to see how subtle (Or should it be Un-subtle) they rip off all the other films . I am great fan of un-serious spoof films, but i am also a fan of the real thing, and with this films, it is hard to decide which is better, the film it mainly rips off (mentioned earlier i this review) or the actual film it is, but also when you are actually making a spoof of comedy films, it actually makes it even harder, but this film carries it off successfully . The two garbage men are \u001b[34mso\u001b[0m \u001b[34mfunny\u001b[0m, it reminds me of a TV sketch show in the UK called 'Little Britain' . This film is a must for your collection and is one of the \u001b[34mbest\u001b[0m, \u001b[34mmost\u001b[0m \u001b[34mentertaining\u001b[0m, \u001b[34mfunniest\u001b[0m, \u001b[34mbest\u001b[0m storyline, National Lampoon's film to date!!\n",
      "****************************************************************************************************\n",
      "doc_id:17373\n",
      "doc_label:positive\n",
      "I have looked forward to seeing this since I first saw it listed in her work . Finally found it yesterday 2/13/02 on Lifetime Movie Channel . Jim Larson's comments about it being a \"sweet funny story of 2 people crossing paths\" were dead on . Writers probably shouldn't get a bonus, everyone else SRO for making the movie . Anybody who appreciates a romantic Movie SHOULD SEE IT . Natasha's screen presence is so warm and her smile so electric, to say nothing of her beauty, that anything she is in goes on my favorite list . Her TV and print interviews that I have seen are just as \u001b[34mrefreshing\u001b[0m and \u001b[34mwell\u001b[0m \u001b[34mworth\u001b[0m looking for . God Bless her, her family and future endeavors . This movie doesn't seem to available in DVD or video yet, but I would be the first to buy it and I think others would too.\n",
      "****************************************************************************************************\n",
      "doc_id:17941\n",
      "doc_label:positive\n",
      "It seems to be a perfect day for swimming . A normal family wants to gain advantage from it and takes a trip to the beach . Unfortunately it happens that the father is trapped under a pier and neither his wife nor the small son is able to help him out of this - whereas the tide is rising . The woman (Barbara Stanwyck) takes the car and searches for help . John Sturges' short movie (69 minutes) is powerful because of unanswered questions . Stanwyck finds a guy who could help, but there is a price she has to pay for this . There is a \u001b[34mdouble\u001b[0m \u001b[34mquestion\u001b[0m the movie poses . How far would you go to help the man that you love, and on the other hand - observing Stanwyck's behaviors towards the stranger - does she really love her husband? Like a \u001b[34mgood\u001b[0m short story this movie leaves the viewer to himself with questions he can only answer himself.\n",
      "****************************************************************************************************\n",
      "doc_id:20753\n",
      "doc_label:positive\n",
      "This group of English pros are a \u001b[34mpleasure\u001b[0m to watch . The supporting cast could form a series of their own . It's a seen before love tiangle between the head of surgery, his wife, and a new pretty boy surgery resident . Only the superior acting skills of Francesca Annis, Michael Kitchen, and the sexy Robson Greene lift this from the trash category to a very enjoyable \"romp\". The only quibble is that it's hard to accept that the smoldering Francesca Annis would fall in love and actually marry Michael Kitchen, who like me, is hardly an international, or even a British sex symbol . You can readily understand why Robson Green would light her fire, with apologies to the \"Doors\". The guy who almost steals the show with a great \"laid back\" performance is Owen's father David Bradley . Watch him in \"The Way We Live Now\", in a completely different performance, to get an idea of his range . Daniela Nardini as Kitchen's secretary, sometime sex toy, is hard to forget as the spurned mistress who makes Kitchen sorry he ever looked at her great body . Conor Mullen, and Julian Rhind-Tutt, as Green's sidekick surgery buddies as I've said could have their own series . They are that \u001b[34mgood\u001b[0m . The whole thing is a great deal of \u001b[34mfun\u001b[0m, and I \u001b[34mheartily\u001b[0m \u001b[34mrecommend\u001b[0m \u001b[34mit\u001b[0m, and thank you imdbman for letting the paying customers have their say in this fascinating venue.\n",
      "****************************************************************************************************\n",
      "doc_id:2616\n",
      "doc_label:positive\n",
      "This movie was, as Homer Simpson would have put it, \"more \u001b[34mboring\u001b[0m than church .\" Maybe I don't understand it well enough, and I thought it started out pretty well, but after (START OF SPOILER) Hermann Braun is sent to jail and Maria starts working/sleeping with her boss it just started to \u001b[34mdrag\u001b[0m, and I \u001b[34mstruggled\u001b[0m to keep awake . Again, maybe it symbolizes something, but the explosion at the end seemed \u001b[34mvery\u001b[0m \u001b[34mforced\u001b[0m and \u001b[34mout\u001b[0m \u001b[34mof\u001b[0m \u001b[34mplace\u001b[0m . (END OF SPOILER). In the end, I \u001b[34mfail\u001b[0m to see why others think it's so great, as I found it extremely \u001b[34mboring\u001b[0m . By the way, I did not watch this movie by my own free will, as I was \u001b[34mrequired\u001b[0m to see it for a Film class.\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "random_index = random.sample(doc_ids,5)\n",
    "\n",
    "\n",
    "for doc_id in random_index[-10:]:\n",
    "\n",
    "    label = 'negative' if data[doc_id] == 0 else 'positive'\n",
    "    print(f'doc_id:{doc_id}')\n",
    "    print(f'doc_label:{label}')\n",
    "\n",
    "    ## select text\n",
    "    original = data[doc_id]['text']\n",
    "    token_original = word_tokenize(original)\n",
    "\n",
    "    rationale_pos = rationale_positions[doc_id]\n",
    "\n",
    "    highlighted = Visualise_rationales(original,_,rationale_pos,visualise_all=True)\n",
    "    print(highlighted)\n",
    "    print('*'*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate semi-factual examples based on non-rationales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## an example of generating semi-factual data off-line by RoBERTa\n",
    "unmasker = pipeline('fill-mask', model='roberta-base',top_k=15,device=0)\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lujinghui\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aec4ab687f145ddaf36e16e7bb4de6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"I found this to be a profoundly amusing dark comedy . Brosnan is genius; as anyone will now testify, he is not to be seen in the bond role . Kinnear was as charismatic and as funny as someone could have been in the film . I don't know if I've laughed as hard during any movie! What an unexpected pleasure! My favourite line would be' I feel like a bangkok hooker on a Sunday - the navy left town' . Brosnan delivered this very un-bond line with such unexpected comedic finesse . I was also very impressed with Hope Davis's performance . It seems the everyone in this movie branched out from their previous work to such a degree that it actually improved the comedy . If you saw the dark and hilarious 'The Weather Man', you will definitely like this . I voted 10.\", \"I found this to be a profoundly amusing dark satire . Brosnan is genius; as anyone will now testify, he is not to be pigeonholed in the bond role . Kinnear was as charismatic and as funny as anyone could have been in the role . I don't know if I've laughed as hard during any movie! What an unexpected pleasure! My favourite line probably be' I feel like a bangkok hooker for a Sunday since the navy left town' . Brosnan delivered this very un-bond line with such unexpected comedic finesse . I was also very impressed with Hope Davis's performance . It seems like everyone in this movie branched out since their previous work to such a degree that it actually improved the comedy . If you liked the dark and hilarious 'The Weather Man', you will definitely like this . I voted 10.\", \"I found this to be a profoundly amusing dark comedy . Brosnan is genius; as anyone will now testify, he is not to be pigeonholed in the bond role . Kinnear was as charismatic and as funny as anyone could have been in the role . Still don't know if I've laughed as hard during any movie! What an unexpected pleasure! My favourite line would …' I feel like a bangkok hooker many a Sunday because the navy left town' . Brosnan delivered this very un-bond line with such unexpected comedic finesse . I was also very impressed with Hope Davis's performance . It seems like everyone in this movie branched out from all previous work to such a degree that it actually improved the comedy . If you liked the dark and hilarious 'The Weather Man', you will definitely like this . 8 voted 10.\"]\n"
     ]
    }
   ],
   "source": [
    "doc_idx = '16740'\n",
    "ban_list = positive_terms + negative_terms\n",
    "text = data[doc_idx]['text']\n",
    "candidates = Generate_semi_factual(text,unmasker,tokenizer,rationale_positions[doc_idx],ban_list,num_candidates=16,percentage=0.05)\n",
    "\n",
    "print(candidates[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lujinghui\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98957203c7d54d5fa3e2c8ceed202616",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lujinghui\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:39: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee2f88dfd9846dea526f0f9bbefc27c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6caba0516a5c4107b666ce667d0ed17d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca112c6fb5b74f868bede8e2a6767f51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d9e1747e08b44f594eacf13e2dc6f94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f550745913240b09ad7247171a48bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e26877d5aee415bacfd85ce94100d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03e9d9156dd84c04ba0601bf8cb7406a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a582fe8be69a4a29aa51bf79168e7016",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "138932a6d83d40c6bb9899f8cd1ee69d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "789c9813bcae4fbf8e03b4efe1e391dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## generating semi-factual/random-replacement data off-line by RoBERTa just 10 for saving time\n",
    "augmented_data = {}\n",
    "for doc_id in tqdm_notebook(list(rationale_positions.keys())[:10]):\n",
    "    ori_text = data[doc_id]['text']\n",
    "    ori_label = data[doc_id]['label']   \n",
    "    \n",
    "    ## using random replacement or not\n",
    "    if not args.is_random_replace:\n",
    "        candidates = Generate_semi_factual(ori_text,unmasker,tokenizer,rationale_positions[doc_id],ban_list,percentage=0.05,num_candidates=16)\n",
    "    else:\n",
    "        candidates = Random_replacement(ori_text,unmasker,tokenizer,ban_list,percentage=0.05,num_candidates=16)\n",
    "        \n",
    "    augmented_data[doc_id] = {'candidates':candidates,'label':ori_label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save to local\n",
    "\n",
    "if not args.is_random_replace:\n",
    "    with open(\"./datasets/IMDb/semi_factual_augmented_examples.json\", \"w\") as file_name:\n",
    "        json.dump(augmented_data, file_name)    \n",
    "\n",
    "else:\n",
    "    with open(\"./datasets/IMDb/random_replace_examples.json\", \"w\") as file_name:\n",
    "        json.dump(augmented_data, file_name)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load from local to check\n",
    "# with open(\"./datasets/IMDb/semi_factual_augmented_examples.json\", \"r\") as file_name:\n",
    "#     augmented_data = json.load(file_name)\n",
    "    \n",
    "with open(\"../over_generalisation/step0_LR_5e-6/IMDb_og_human_trainer_2019_25_7/augmented_step1.json\", \"r\") as file_name:\n",
    "    augmented_data = json.load(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(augmented_data['8518']['candidates'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[:-3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints\n",
      "augmented_step1.json\n",
      "copied augmented_step1.json\n",
      "checkpoint-700\n",
      "false_rationale.txt\n",
      "copied false_rationale.txt\n",
      "false_rationles.tsv\n",
      "copied false_rationles.tsv\n",
      "keys.txt\n",
      "copied keys.txt\n",
      "loggings.json\n",
      "copied loggings.json\n",
      "missing_rationales_augmented_step1.json\n",
      "copied missing_rationales_augmented_step1.json\n",
      "new_keys.txt\n",
      "copied new_keys.txt\n",
      "process_output.txt\n",
      "copied process_output.txt\n",
      "Untitled.ipynb\n",
      "copied Untitled.ipynb\n",
      "copied Untitled.ipynb\n",
      "augmented_step1.json\n",
      "copied augmented_step1.json\n",
      "checkpoint-100\n",
      "false_rationale.txt\n",
      "copied false_rationale.txt\n",
      "false_rationles.tsv\n",
      "copied false_rationles.tsv\n",
      "keys.txt\n",
      "copied keys.txt\n",
      "loggings.json\n",
      "copied loggings.json\n",
      "missing_rationales_augmented_step1.json\n",
      "copied missing_rationales_augmented_step1.json\n",
      "missing_rationales_output.txt\n",
      "copied missing_rationales_output.txt\n",
      "new_keys.txt\n",
      "copied new_keys.txt\n",
      "process_output.txt\n",
      "copied process_output.txt\n",
      "copied process_output.txt\n",
      "augmented_step1.json\n",
      "copied augmented_step1.json\n",
      "checkpoint-200\n",
      "false_rationale.txt\n",
      "copied false_rationale.txt\n",
      "false_rationles.tsv\n",
      "copied false_rationles.tsv\n",
      "keys.txt\n",
      "copied keys.txt\n",
      "loggings.json\n",
      "copied loggings.json\n",
      "missing_rationales_augmented_step1.json\n",
      "copied missing_rationales_augmented_step1.json\n",
      "missing_rationales_output.txt\n",
      "copied missing_rationales_output.txt\n",
      "new_keys.txt\n",
      "copied new_keys.txt\n",
      "process_output.txt\n",
      "copied process_output.txt\n",
      "copied process_output.txt\n",
      "augmented_step1.json\n",
      "copied augmented_step1.json\n",
      "checkpoint-200\n",
      "false_rationale.txt\n",
      "copied false_rationale.txt\n",
      "false_rationles.tsv\n",
      "copied false_rationles.tsv\n",
      "keys.txt\n",
      "copied keys.txt\n",
      "loggings.json\n",
      "copied loggings.json\n",
      "missing_rationales_augmented_step1.json\n",
      "copied missing_rationales_augmented_step1.json\n",
      "missing_rationales_output.txt\n",
      "copied missing_rationales_output.txt\n",
      "new_keys.txt\n",
      "copied new_keys.txt\n",
      "process_output.txt\n",
      "copied process_output.txt\n",
      "copied process_output.txt\n",
      "augmented_step1.json\n",
      "copied augmented_step1.json\n",
      "checkpoint-200\n",
      "false_rationale.txt\n",
      "copied false_rationale.txt\n",
      "false_rationles.tsv\n",
      "copied false_rationles.tsv\n",
      "keys.txt\n",
      "copied keys.txt\n",
      "loggings.json\n",
      "copied loggings.json\n",
      "missing_rationales_augmented_step1.json\n",
      "copied missing_rationales_augmented_step1.json\n",
      "missing_rationales_output.txt\n",
      "copied missing_rationales_output.txt\n",
      "new_keys.txt\n",
      "copied new_keys.txt\n",
      "process_output.txt\n",
      "copied process_output.txt\n",
      "copied process_output.txt\n",
      "augmented_step1.json\n",
      "copied augmented_step1.json\n",
      "checkpoint-400\n",
      "false_rationale.txt\n",
      "copied false_rationale.txt\n",
      "false_rationles.tsv\n",
      "copied false_rationles.tsv\n",
      "keys.txt\n",
      "copied keys.txt\n",
      "loggings.json\n",
      "copied loggings.json\n",
      "missing_rationales_augmented_step1.json\n",
      "copied missing_rationales_augmented_step1.json\n",
      "missing_rationales_output.txt\n",
      "copied missing_rationales_output.txt\n",
      "new_keys.txt\n",
      "copied new_keys.txt\n",
      "process_output.txt\n",
      "copied process_output.txt\n",
      "copied process_output.txt\n",
      "augmented_step1.json\n",
      "copied augmented_step1.json\n",
      "checkpoint-700\n",
      "false_rationale.txt\n",
      "copied false_rationale.txt\n",
      "false_rationles.tsv\n",
      "copied false_rationles.tsv\n",
      "keys.txt\n",
      "copied keys.txt\n",
      "loggings.json\n",
      "copied loggings.json\n",
      "missing_rationales_augmented_step1.json\n",
      "copied missing_rationales_augmented_step1.json\n",
      "missing_rationales_output.txt\n",
      "copied missing_rationales_output.txt\n",
      "new_keys.txt\n",
      "copied new_keys.txt\n",
      "process_output.txt\n",
      "copied process_output.txt\n",
      "copied process_output.txt\n",
      "augmented_step1.json\n",
      "copied augmented_step1.json\n",
      "checkpoint-200\n",
      "false_rationale.txt\n",
      "copied false_rationale.txt\n",
      "false_rationles.tsv\n",
      "copied false_rationles.tsv\n",
      "keys.txt\n",
      "copied keys.txt\n",
      "loggings.json\n",
      "copied loggings.json\n",
      "missing_rationales_augmented_step1.json\n",
      "copied missing_rationales_augmented_step1.json\n",
      "missing_rationales_output.txt\n",
      "copied missing_rationales_output.txt\n",
      "new_keys.txt\n",
      "copied new_keys.txt\n",
      "process_output.txt\n",
      "copied process_output.txt\n",
      "copied process_output.txt\n",
      "augmented_step1.json\n",
      "copied augmented_step1.json\n",
      "checkpoint-200\n",
      "false_rationale.txt\n",
      "copied false_rationale.txt\n",
      "false_rationles.tsv\n",
      "copied false_rationles.tsv\n",
      "keys.txt\n",
      "copied keys.txt\n",
      "loggings.json\n",
      "copied loggings.json\n",
      "missing_rationales_augmented_step1.json\n",
      "copied missing_rationales_augmented_step1.json\n",
      "missing_rationales_output.txt\n",
      "copied missing_rationales_output.txt\n",
      "new_keys.txt\n",
      "copied new_keys.txt\n",
      "process_output.txt\n",
      "copied process_output.txt\n",
      "copied process_output.txt\n",
      "augmented_step1.json\n",
      "copied augmented_step1.json\n",
      "checkpoint-100\n",
      "false_rationles.tsv\n",
      "copied false_rationles.tsv\n",
      "keys.txt\n",
      "copied keys.txt\n",
      "loggings.json\n",
      "copied loggings.json\n",
      "missing_rationales_augmented_step1.json\n",
      "copied missing_rationales_augmented_step1.json\n",
      "missing_rationales_output.txt\n",
      "copied missing_rationales_output.txt\n",
      "new_keys.txt\n",
      "copied new_keys.txt\n",
      "process_output.txt\n",
      "copied process_output.txt\n",
      "copied process_output.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "for seed in range(2019,2029):\n",
    "\n",
    "    source_folder = f\"../over_generalisation/step0_LR_5e-6/IMDb_og_human_trainer_{seed}_25_7/\"\n",
    "    destination_folder = f\"./AL_results/AL_step0_IMDb_trainer_{seed}_25/\"\n",
    "\n",
    "    # fetch all files\n",
    "    for file_name in os.listdir(source_folder):\n",
    "        print(file_name)\n",
    "        if file_name.startswith('checkpoint'):\n",
    "            pass\n",
    "        else:\n",
    "            source = source_folder + file_name\n",
    "            destination = destination_folder + file_name\n",
    "\n",
    "            if not os.path.exists(destination_folder):\n",
    "                os.mkdir(destination_folder)\n",
    "\n",
    "            if os.path.isfile(source):\n",
    "                shutil.copy(source, destination)\n",
    "                print('copied', file_name)\n",
    "    # construct full file path\n",
    "    source = source_folder + file_name\n",
    "    destination = destination_folder + file_name\n",
    "    # copy only files\n",
    "    if os.path.isfile(source):\n",
    "        shutil.copy(source, destination)\n",
    "        print('copied', file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
