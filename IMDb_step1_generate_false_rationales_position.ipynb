{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(2019)\n",
    "os.environ['TRANSFORMERS_CACHE'] = 'D:\\\\python_pkg_data\\\\huggingface\\\\transformers'\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import numpy as np \n",
    "np.random.seed(2019)\n",
    "import random\n",
    "random.seed(2019)\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(2019)\n",
    "torch.cuda.manual_seed_all(2019)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "import transformers\n",
    "\n",
    "\n",
    "\n",
    "from datasets import load_metric,load_dataset,Value\n",
    "import csv\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.data.path.append('D:\\\\python_pkg_data\\\\nltk_data')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "import ast\n",
    "import glob\n",
    "import shutil\n",
    "\n",
    "import importlib\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import Softmax\n",
    "from termcolor import colored\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import html\n",
    "from IPython.core.display import display, HTML\n",
    "import more_itertools as mit\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'ori_train_dir':'./datasets/IMDb/orig/train.tsv',\n",
    "    'gpu_device':0,\n",
    "    'tokenizer': transformers.AutoTokenizer.from_pretrained('roberta-base'),\n",
    "    'dataset_cache_dir':\"D:\\\\python_pkg_data\\\\huggingface\\\\Datasets\", ## local directory for datasets\n",
    "    'num_per_class': 25,                                              ## number of examples per class for initial training set\n",
    "    'save_dir': './SF_results/IMDb_step0_sf_trainer',                                   ##directory for saving models\n",
    "    'num_per_example':7,\n",
    "    'labelled_pos': './datasets/IMDb/human_labelled/positives.json',\n",
    "    'labelled_neg':'./datasets/IMDb/human_labelled/negatives.json',\n",
    "    'supplement': './datasets/IMDb/human_labelled/supplement_rationales.tsv',\n",
    "    'al_dir': './AL_results/AL_step0_IMDb_trainer',\n",
    "    'train_random_seed':2019\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\lujinghui\\.cache\\huggingface\\modules\\datasets_modules\\metrics\\accuracy\\3e9ee15abf476145152fe4e9a9c1463ff95d3d65cdc555be9cfe061bdaeb1a14 (last modified on Wed Oct 27 09:07:35 2021) since it couldn't be found locally at accuracy, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def html_escape(text):\n",
    "    return html.escape(text)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "class CustomerDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "\n",
    "def import_data(directory):\n",
    "    \n",
    "    data_pos = []\n",
    "    data_neg = []\n",
    "    with open(directory,errors='ignore') as file:\n",
    "        file = csv.reader(file, delimiter=\"\\t\")\n",
    "        for idx,row in enumerate(file):\n",
    "            if idx!=0:\n",
    "                if row[0] == 'Negative':\n",
    "                    data_neg.append({'idx':idx,'text':row[1],'label':0})\n",
    "                else:\n",
    "                    data_pos.append({'idx':idx,'text':row[1],'label':1})\n",
    "            \n",
    "    return data_neg,data_pos\n",
    "\n",
    "def import_paired_data(directory,original_texts):\n",
    "    paired_data = {}\n",
    "    with open(directory,errors='ignore') as file:\n",
    "        file = csv.reader(file, delimiter=\"\\t\")\n",
    "        for idx,row in enumerate(file):\n",
    "           \n",
    "            if idx!=0:\n",
    "                if row[2] not in paired_data.keys():\n",
    "                    paired_data[row[2]] = []\n",
    "\n",
    "                    if row[1] in original_texts:\n",
    "                        paired_data[row[2]].append({'text':row[1],'label':0 if row[0]=='Negative'else 1,'ori_flag':1})\n",
    "                      \n",
    "                    else:\n",
    "                        paired_data[row[2]].append({'text':row[1],'label':0 if row[0]=='Negative'else 1,'ori_flag':0})\n",
    "                else:\n",
    "                    if row[1] in original_texts:\n",
    "                        paired_data[row[2]].append({'text':row[1],'label':0 if row[0]=='Negative'else 1,'ori_flag':1})\n",
    "                    else:\n",
    "                        paired_data[row[2]].append({'text':row[1],'label':0 if row[0]=='Negative'else 1,'ori_flag':0})\n",
    "                        \n",
    "    return paired_data\n",
    "\n",
    "def visualise_rationales(original,rationale_spans,rationale_pos,visualise_all=False):\n",
    "    \n",
    "    if visualise_all:\n",
    "        highlighted = []\n",
    "        for idx,term in enumerate(word_tokenize(original)):\n",
    "            if idx in rationale_pos:\n",
    "                highlighted.append(colored(term,'blue'))\n",
    "            else:\n",
    "                highlighted.append(term)\n",
    "            \n",
    "        return TreebankWordDetokenizer().detokenize(highlighted)\n",
    "                \n",
    "    else:\n",
    "        highlights = []\n",
    "        for span in rationale_spans:\n",
    "            highlighted = []\n",
    "            for idx,term in enumerate(word_tokenize(original)):\n",
    "                if idx in span:\n",
    "                    highlighted.append(colored(term,'blue'))\n",
    "                else:\n",
    "                    highlighted.append(term)\n",
    "                    \n",
    "            highlights.append(TreebankWordDetokenizer().detokenize(highlighted))\n",
    "        \n",
    "        return highlights\n",
    "    \n",
    "def visualise_rationales_html(original,rationale_spans,rationale_pos,visualise_all=False):\n",
    "    \n",
    "    if visualise_all:\n",
    "        highlighted = []\n",
    "        for idx,term in enumerate(word_tokenize(original)):\n",
    "            if idx in rationale_pos:\n",
    "                highlighted.append('<font color=\"blue\">' + html_escape(term) + '</font>')\n",
    "            else:\n",
    "                highlighted.append(term)\n",
    "            \n",
    "        return TreebankWordDetokenizer().detokenize(highlighted)\n",
    "                \n",
    "    else:\n",
    "        highlights = []\n",
    "        for span in rationale_spans:\n",
    "            highlighted = []\n",
    "            for idx,term in enumerate(word_tokenize(original)):\n",
    "                if idx in span:\n",
    "                    highlighted.append('<font color=\"blue\">' + html_escape(term) + '</font>')\n",
    "                else:\n",
    "                    highlighted.append(term)\n",
    "                    \n",
    "            highlights.append(TreebankWordDetokenizer().detokenize(highlighted))\n",
    "        \n",
    "        return highlights\n",
    "    \n",
    "\n",
    "def detect_rationale_spans(non_rationale_pos,text_length,max_length=1):\n",
    "    \n",
    "    rationale_spans = []\n",
    "    \n",
    "    rationale_pos = list(set([i for i in range(text_length)])-set(non_rationale_pos))\n",
    "    rationale_pos.sort()\n",
    " \n",
    "    for k, g in groupby(enumerate(rationale_pos),lambda ix : ix[0] - ix[1]): \n",
    "        span = list(map(itemgetter(1), g))\n",
    "        if len(span) <= max_length:\n",
    "            rationale_spans.append(span)\n",
    "    \n",
    "    \n",
    "      \n",
    "    return rationale_spans, rationale_pos\n",
    "\n",
    "def identify_important_terms(token_text):\n",
    "    candidates = []\n",
    "    remove_terms = {}\n",
    "    count = 0\n",
    "    for idx,token in enumerate(token_text):\n",
    "        duplicate = token_text.copy()\n",
    "        remove_terms[count] = {'terms':duplicate[idx],'start_token':idx,'end_token':idx+1}\n",
    "        del duplicate[idx]\n",
    "        count += 1\n",
    "        candidates.append(TreebankWordDetokenizer().detokenize(duplicate))\n",
    "\n",
    "    for idx,token in enumerate(token_text[:-1]):\n",
    "        duplicate = token_text.copy()\n",
    "        remove_terms[count] = {'terms':duplicate[idx:idx+2],'start_token':idx,'end_token':idx+2}\n",
    "        del duplicate[idx:idx+2]\n",
    "        count += 1\n",
    "        candidates.append(TreebankWordDetokenizer().detokenize(duplicate))\n",
    "\n",
    "    for idx,token in enumerate(token_text[:-2]):\n",
    "        duplicate = token_text.copy()\n",
    "        remove_terms[count] = {'terms':duplicate[idx:idx+3],'start_token':idx,'end_token':idx+3}\n",
    "        del duplicate[idx:idx+3]\n",
    "        count += 1\n",
    "        candidates.append(TreebankWordDetokenizer().detokenize(duplicate))\n",
    "\n",
    "\n",
    "    candidates.append(text)\n",
    "    \n",
    "    return candidates, remove_terms\n",
    "\n",
    "\n",
    "def get_rationale_spans(model,text,label,topk=15):\n",
    "    \n",
    "#     text = imdb_texts[example_idx]\n",
    "#     label = imdb_labels[example_idx]\n",
    "    token_text = word_tokenize(text)\n",
    "\n",
    "    candidates, remove_terms = identify_important_terms(token_text)\n",
    "\n",
    "    candidates_label = [label]*len(candidates)\n",
    "\n",
    "    candidates_encodings = args['tokenizer'](candidates, truncation=True, padding=True)\n",
    "    candidates_dataset = CustomerDataset(candidates_encodings, candidates_label)\n",
    "    candidates_dataloader = DataLoader(candidates_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    output_logits = []\n",
    "    for batch in tqdm_notebook(candidates_dataloader):\n",
    "        batch = {k: v.cuda(args['gpu_device']) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "        output_logits.append(logits)\n",
    "\n",
    "    outputs = torch.cat(output_logits)\n",
    "    sm = Softmax(dim=1)\n",
    "    outputs = sm(outputs)\n",
    "\n",
    "    results = {}\n",
    "    for idx,score in enumerate(outputs[:-1]):\n",
    "        changes = abs(float(outputs[idx][label]-outputs[-1][label]))\n",
    "        results[idx]=changes\n",
    "\n",
    "    token_id = list(dict(sorted(results.items(), key=lambda item: item[1],reverse=True)).keys())\n",
    "\n",
    "    inferred_spans = []\n",
    "    for ids in token_id[:topk]:\n",
    "#         print(ids, remove_terms[ids]['terms'])\n",
    "        span = [i for i in range(remove_terms[ids]['start_token'],remove_terms[ids]['end_token'])]\n",
    "        inferred_spans.append(span)\n",
    "\n",
    "    inferred_pos = []\n",
    "    for span in inferred_spans:\n",
    "        for number in span:\n",
    "            inferred_pos.append(number)\n",
    "\n",
    "\n",
    "    inferred_pos = list(set(inferred_pos))\n",
    "    \n",
    "    return inferred_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import train data\n",
    "IMDb_data = {}\n",
    "\n",
    "with open(args['ori_train_dir'],errors='ignore') as file:\n",
    "    file = csv.reader(file, delimiter=\"\\t\")\n",
    "    for idx,row in enumerate(file):\n",
    "        if len(row)>0:\n",
    "\n",
    "            if row[0] == 'Negative':\n",
    "                IMDb_data[row[2]] = {'text':row[1],'label':0}\n",
    "            else:\n",
    "                IMDb_data[row[2]] = {'text':row[1],'label':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "855"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_rationales = json.load(open(args['labelled_pos'], 'r'))\n",
    "neg_rationales = json.load(open(args['labelled_neg'], 'r'))\n",
    "\n",
    "rationale_spans = {}\n",
    "for item in neg_rationales:\n",
    "    key = list(item.keys())[1]\n",
    "    index = list(item.keys())[1][9:-4]\n",
    "    rationale_spans[index] = item[key]\n",
    "    \n",
    "\n",
    "for item in pos_rationales:\n",
    "    key = list(item.keys())[1]\n",
    "    index = list(item.keys())[1][9:-4]\n",
    "    rationale_spans[index] = item[key]\n",
    "\n",
    "rationale_positions = {}\n",
    "\n",
    "train_keys = list(rationale_spans.keys())\n",
    "\n",
    "for key in train_keys:\n",
    "    doc_positions = []\n",
    "    positions = rationale_spans[key]\n",
    "    for span in positions:\n",
    "        start = span['start_token']\n",
    "        end = span['end_token']\n",
    "        doc_positions = doc_positions +[i for i in range(start,end)]\n",
    "        \n",
    "    rationale_positions[key] = doc_positions\n",
    "\n",
    "len(rationale_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplement_rationales = {}\n",
    "with open(args['supplement'],'r') as file:\n",
    "    file = csv.reader(file, delimiter='\\t')\n",
    "    for idx,row in enumerate(file):\n",
    "        supplement_rationales[row[0]] = ast.literal_eval(row[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rationale_positions.update(supplement_rationales)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most uncertainty examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_keys_dir = f\"{args['save_dir']}_{args['train_random_seed']}_{args['num_per_class']}_{args['num_per_example']}/keys.txt\"\n",
    "new_keys_dir = f\"{args['save_dir']}_{args['train_random_seed']}_{args['num_per_class']}_{args['num_per_example']}/new_keys.txt\"\n",
    "\n",
    "all_keys = []\n",
    "with open(train_keys_dir,'r') as file:\n",
    "    for key in file.readlines():\n",
    "        all_keys.append(key[:-1])\n",
    "        \n",
    "with open(new_keys_dir,'r') as file:\n",
    "    for key in file.readlines():\n",
    "        all_keys.append(key[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Causal Terms via Fine-tuned RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "previous model ./step0_LR_5e-6/IMDb_og_human_trainer_2028_25_7\\checkpoint-100\n"
     ]
    }
   ],
   "source": [
    "model_dir = glob.glob(f\"{args['save_dir']}_{args['train_random_seed']}_{args['num_per_class']}_{args['num_per_example']}/checkpoint*\")[0]\n",
    "print(f\"previous model {model_dir}\")\n",
    "model = transformers.AutoModelForSequenceClassification.from_pretrained(model_dir, num_labels=2).cuda(args['gpu_device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14437 positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lujinghui\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:179: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fac54369712440b78461ef392c5c0adf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bette Midler is again \u001b[34mDivine\u001b[0m \u001b[34m!\u001b[0m \u001b[34mRaunchily\u001b[0m \u001b[34mhumorous\u001b[0m \u001b[34m.\u001b[0m \u001b[34mIn\u001b[0m \u001b[34mlove\u001b[0m \u001b[34mwith\u001b[0m \u001b[34mBurlesque\u001b[0m \u001b[34m.\u001b[0m Capable of bringing you down to tears either with old jokes with new dresses or merely with old songs with more power & punch than ever . All in All Singing new ballads, power-singing the good old/perennial ones such as \"The Rose\"; \"Stay With Me\" and yes, even \"Wind Beneath My Wings\". The best way \u001b[34mto\u001b[0m \u001b[34mappreciate\u001b[0m \u001b[34mthe\u001b[0m Divine Miss M has always been libe - since this is the next best thing to it, I strongly recommended to all with a mixture of adult wide-eyed enchantment and appreciation and a child's mischievous wish for pushing all boundaries!\n"
     ]
    }
   ],
   "source": [
    "# key = '14437'\n",
    "\n",
    "\n",
    "# text = IMDb_data[key]['text']\n",
    "# label = IMDb_data[key]['label']\n",
    "# token_text = word_tokenize(text)\n",
    "\n",
    "# print(key, 'negative' if label==0 else 'positive')\n",
    "\n",
    "# generated_rationales_spans = get_rationale_spans(model,text,label)\n",
    "\n",
    "\n",
    "\n",
    "# highlighted = visualise_rationales(text,_,generated_rationales_spans,visualise_all=True)\n",
    "# print(highlighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bette Midler is \u001b[34magain\u001b[0m Divine! \u001b[34mRaunchily\u001b[0m \u001b[34mhumorous\u001b[0m . In love with Burlesque . Capable of \u001b[34mbringing\u001b[0m you \u001b[34mdown\u001b[0m to \u001b[34mtears\u001b[0m either with old jokes with new dresses or merely with old songs with \u001b[34mmore\u001b[0m power & punch than ever . All in All Singing \u001b[34mnew\u001b[0m ballads, \u001b[34mpower-singing\u001b[0m the \u001b[34mgood\u001b[0m old/perennial ones such as \"The Rose\"; \"Stay With Me\" and yes, even \"Wind Beneath My Wings\". The best way to appreciate the Divine Miss M has always been libe - since this is \u001b[34mthe\u001b[0m \u001b[34mnext\u001b[0m \u001b[34mbest\u001b[0m \u001b[34mthing\u001b[0m \u001b[34mto\u001b[0m it, I \u001b[34mstrongly\u001b[0m \u001b[34mrecommended\u001b[0m to all with a mixture of adult wide-eyed \u001b[34menchantment\u001b[0m and \u001b[34mappreciation\u001b[0m and a child's \u001b[34mmischievous\u001b[0m wish for pushing all boundaries!\n"
     ]
    }
   ],
   "source": [
    "# rationale_pos = rationale_positions[key]\n",
    "\n",
    "\n",
    "# highlighted = visualise_rationales(text,_,rationale_pos,visualise_all=True)\n",
    "# print(highlighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokened_text = word_tokenize(text)\n",
    "# for idx,token in enumerate(tokened_text):\n",
    "#     if token == \"pain\":\n",
    "#         print(idx, token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['to', 'appreciate', 'the']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokened_text[79:82]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rationales = []\n",
    "for example_idx in tqdm_notebook(all_keys):\n",
    "    \n",
    "    \n",
    "    \n",
    "    text = IMDb_data[example_idx]['text']\n",
    "    label = IMDb_data[example_idx]['label']\n",
    "    token_text = word_tokenize(text)\n",
    "    \n",
    "    print(example_idx, 'negative' if label==0 else 'positive')\n",
    "    \n",
    "    generated_rationales_spans = get_rationale_spans(model,text,label)\n",
    "    \n",
    "    \n",
    "    \n",
    "    highlighted = visualise_rationales(text,_,generated_rationales_spans,visualise_all=True)\n",
    "    print(highlighted)\n",
    "\n",
    "    # combine consectutive positions\n",
    "    generated_rationales_spans = [list(group) for group in mit.consecutive_groups(generated_rationales_spans)]\n",
    "\n",
    "    generated_important_words = {}\n",
    "    for idx,span in enumerate(generated_rationales_spans):\n",
    "        span_text = [token_text[pos] for pos in span]\n",
    "        span_text = TreebankWordDetokenizer().detokenize(span_text)\n",
    "        generated_important_words[idx] = {'positions':span,'label':False,'text':span_text}\n",
    "\n",
    "#     print(example_idx, paired_train_data[example_idx][0]['label'] if paired_train_data[example_idx][0]['ori_flag'] == 1 else paired_train_data[example_idx][1]['label'])\n",
    "            ## select text\n",
    "    original = IMDb_data[example_idx]['text']\n",
    "    token_original = word_tokenize(original)\n",
    "\n",
    "    ## specify non-rationale spans\n",
    "#     non_rationale_pos = non_rationales[example_idx]\n",
    "\n",
    "\n",
    "    ## detect rationale spans\n",
    "#     rationale_spans,rationale_pos = detect_rationale_spans(non_rationale_pos,len(token_original),max_length=1000)\n",
    "    rationale_pos = rationale_positions[example_idx]\n",
    "\n",
    "\n",
    "    highlighted = visualise_rationales(text,_,rationale_pos,visualise_all=True)\n",
    "    print(highlighted)\n",
    "\n",
    "    rationale_pos = [list(group) for group in mit.consecutive_groups(rationale_pos)]\n",
    "\n",
    "    for key in generated_important_words.keys():\n",
    "        generated_span = generated_important_words[key]['positions']  \n",
    "        for span in rationale_pos:\n",
    "\n",
    "            if bool(set(span) & set(generated_span)):\n",
    "                generated_important_words[key]['label'] = True\n",
    "                break\n",
    "\n",
    "    false_important_pos = []\n",
    "\n",
    "    for key in generated_important_words.keys():\n",
    "        if generated_important_words[key]['label'] == False:\n",
    "            for number in generated_important_words[key]['positions']:\n",
    "                false_important_pos.append(number)\n",
    "\n",
    "\n",
    "    rationales.append({'Batch':example_idx,'Positions':false_important_pos})\n",
    "\n",
    "    highlighted = visualise_rationales(text,_,false_important_pos,visualise_all=True)\n",
    "    print(highlighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = f\"{args['save_dir']}_{args['train_random_seed']}_{args['num_per_class']}_{args['num_per_example']}/false_rationles.tsv\"\n",
    "with open(output_dir, 'wt',errors='ignore',newline='') as out_file:\n",
    "    tsv_writer = csv.writer(out_file, delimiter='\\t')\n",
    "    tsv_writer.writerow(['Batch','Positions'])\n",
    "    for term in rationales:\n",
    "        tsv_writer.writerow([term['Batch'],term['Positions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
